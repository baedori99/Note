---
title: Web Crawling Practice
create_date: 2024-06-10 13:06 - 2024-06-10 13:06
draft: true
---
## 멜론 차트 크롤링(Melon Chart Crawling)

>[!Todo]
>1. requests or urllib로 html 불러오기
>2. soup = BeautifulSoup(html)

### 1. requests or urllib로 html 불러오기

>[!Reference]
>[requeset 모듈 문서](https://docs.python-requests.org/en/latest/user/quickstart/)

- `requests`: HTTP 요청을 보내는 모듈
```python
import requests # HTTP 요청을 보내는 모듈

url = "https://www.melon.com/chart/index.htm"
# 현재 사용자가 어떤 클라이언트(운영체제와 브라우저 같은 것)를 이용해 요청을 보냈는지 확인
req_header = {'user-agent': 'Chrome/125.0.6422.142'}

res = requests.get(url, headers= req_header)
html = res.text
```

### 2. soup = BeautifulSoup(html)

>[!Reference]
>[BeautifulSoup 문서](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

- `BeautifulSoup`: HTML과 XML 파일로부터 데이터를 뽑아내기 위한 파이썬 라이브러리
```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(html)
```

---
>[!Memo]
>- 하나씩 정보를 끌어다와서 확인 후 한곡의 모든 항목이 끝나면 딕셔너리로 모은 뒤 for loop를 사용하여 100개 출력하기

[[과제 공부용#.find, .find_all 함수 이해하기|.find, .find_all 함수]]

---
## 교보문고 크롤링

- 교보문고 API를 통해서 데이터 받기
```python
import requests as req
import urllib.parse
from bs4 import BeautifulSoup

# def kyobo_crawling(keyword):#, page):
keyword = "파이썬"
encrypt_word = urllib.parse.quote(keyword)

url = "https://search.kyobobook.co.kr/search?keyword="+ encrypt_word + "&target=total&gbCode=TOT&page=1"#{}".format(page)

req_header = {'user_agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}

res = req.get(url)
html = res.text
soup = BeautifulSoup(html)
```

---
### 책 제목들 가져오기
```python
title = soup.find('li',class_='prod_item').find('a', class_='prod_info').find_all('span')[-1].text
```

- 간결하게 `'li',class_='prod_item'`에 대한 리스트 만든다
- for문 돌리며 title 나오게 하는 초석
```python
rows = soup.find_all('li', class_ = "prod_item")

row = rows[0].find('a',class_ = 'prod_info').find_all('span')[-1].text
row
```

- Output:
```
'파이썬 한권으로 끝내기: 데이터분석전문가(ADP) + 빅데이터분석기사 실기대비'
```

- ==문제발생==
	- `rows`를 출력했을 때 밑에 추천상품까지 뜨게 됨.
- 해결방법
	- 추천상품에는 `prod_list`라는 class가 없는 것을 확인하여 고침
```python
rows = soup.find('ul', class_ = 'prod_list').find_all('li', class_= 'prod_item')
```

- for문을 통해서 책 제목들 가져오기
```python
for i in range(len(rows)):
    row = rows[i].find('a', class_ = 'prod_info').find_all('span')[-1].text
```

---
### 책 가격 가지고 오기
- 책 가격을 가져오기전 생각해봐야하는 것들
	- 어차피 책 제목, 가격, 저자는 for문을 통해서 가지고 올 것이다.
	- 그렇다면 같이 돌려야하기 때문에 `rows`를 통해서 같이 돌려주면 편하다.
	- 그리고 `rows`는 한 칸의 책 제목, 책 저자, 책 가격을 담고 있기 때문에 편하다.

- 책 가격을 for문을 통해 전부 출력하기
```python
for i in range(len(rows)):
    price = rows[i].find('span', class_ = 'price').find('span', class_ = 'val').text
```

---
### 책 저자 가져오기
- 미션
	- 책 저자가 한명 이상인 경우가 있으므로 list로 받아서 가져오기

- ==문제 발생==
	- `author`을 받을 때 1명이상인 것을 고려하여 `span`태그에 있는 `class_='type'`(`저자(글)`을 출력)의 갯수를 구하여 그 갯수만큼 `range()`를 돌려 `author`을 출력할려고 했다.
	- !! 여기서 문제 발생 !! 
		- `class_='type'`의 갯수를 구할 때 for문을 돌려가며 그다음 갯수를 구할려고 했지만 오류가 뜸
- 해결 방법
	- 나래 연구원님께서 그것의 상위 태그에서 불러오라고 하셨다.
	- 하지만 상위 태그(`'div',class_='auto_overflow_inner'`)를 불러보니 책 제목도 출력이 됨
	- `'div',class_='auto_overflow_inner'`이 태그가 책 제목과 책 저자에 같은 이름으로 되어 있었다.
	- 연구원님이 첫번째 요소는 책 제목 두번째 요소는 책 저자로 해서 받으라고 하셨다.

```python
for i in range(len(rows)):
    temp_rows = rows[i].find_all('div', class_='auto_overflow_inner')
    title = temp_rows[0].find('a',class_='prod_info').find_all('span')[-1]
    author = temp_rows[1].find_all('a', class_='author')
```

- 출력할 때 list comprehension을 써서 리스트로 만들고 `','.join()`을 이용해서 리스트를 없애고 출력하였다. 그 결과
```python
for i in range(len(rows)):
    temp_rows = rows[i].find_all('div', class_='auto_overflow_inner')
    title = temp_rows[0].find('a',class_='prod_info').find_all('span')[-1]
    author = temp_rows[1].find_all('a', class_='author')
    price = rows[i].find('span', class_ = 'price').find('span', class_ = 'val').text

    print(','.join([k.text for k in title]), price + "원", "\n", ','.join([x.text for x in author]), "\n")
```

- **이런 방식이 있는 것을 참고!!!**

---
### 페이지
