---
title: 1주차과제_2023년 서울시 미세먼지 현황 보고서 (피드백)
draft: false
---
[[1주차 과제]]
### 과제 형식에 대한 피드백
- 이번 과제는 2023년 서울시 미세먼지 보고서를 코딩을 통해 데이터를 분석하는 것이다.
-  따라서 제목, 소제목, 부연 설명을 더 신경써서 보고서 형식으로 작성해야한다.
---
### 과제 내용에 대한 피드백
![](https://imgur.com/3obnzXo.png)

- 과제 제목
- 제출일/ 제출자
- 내가 해야 할 과제에 대한 세부사항
- 목표
---
## 피드백
- 추가 설명을 통해 다양한 방법으로 데이터를 이용하는 방법에 대해서 설명해 주신다.
### 데이터 불러오기
- 데이터를 불러올때에는 데이터 출처에 대해서 링크를 해두는 것이 좋다.
	- 데이터 출처 쓰는 법
		- ['서울열린데이터광장, 서울시 기간별 일평균 대기환경 정보']'('https://data.seoul.go.kr/dataList/OA-2220/S/1/datasetView.do')' 이런식으로 쓰면 된다.

- index_col은 n번째 column을 index로 설정하는 파라미터이다.
- 데이터의 타입을 사전에 미리 확인하기 위해 index 설정은 나중에 하는 것을 추천
```python
dust_reviews = pd.read_csv("C:/Users/pps/Desktop/배정환(인턴)/기간별_일평균_대기환경_정보_2023년.csv", encoding="CP949")
```
- 피드백
	- 데이터를 불러올 때 주의해야 할 점: `encoding`, 한글이 들어간 경우 깨지는 경우가 있으니 이를 해결할 수 있어야 한다.
	- 원활한 데이터 분석을 위해 변수를 변경할 수 있어야한다. 
	-  Ex) 인덱싱할 때 변수를 쉽게 불러오기 위해 column명을 단순화 하면 다음과 같이 할 수 있다. *특히 시각화 라이브러리는 한글 글꼴이 지원되지 않는 경우가 많기 때문*에 되도록 영어로 수정하는 것이 좋다.
```python
dust_review.columns = ["region","station", "NO2","O3","CO","SO2","PM10","PM2.5"]
```
---
### 데이터 정보 확인하기
- 데이터 타입은 크게 number(`int`, `float`), `datetime`, `object`, `category`로 나눌 수 있다.
```python
dust_reviews.dtypes
```
---
- 데이터의 컬럼 명, 데이터 타입, 전체 데이터 수 등의 정보를 한 번에 이해할 수 있다.
- 즉, `df.columns`, `df.shape`, `df.dtypes`를 따로 하지 않아도 `info`를 통해 알 수 있는 것이다.
- `dust_reviews.info()`를 통해 데이터의 수는 총 9125개이며, _Non-Null Count를 보면 측정값들에 <U>결측치가 존재한다는 것을 유추</U>할 수 있다._ 
```python
dust_reviews.info()
```
---
- `column`의 `datetype`을 변경하는 코드
- 위의 코드를 통해 측정일시의 `datetype`이 정수인 것을 확인할 수 있다. 
- `datetype`를 `datetime`으로 변경하기 위해서는 `df.astype()`으로 `string`으로 바꾼 후 `pd.to_datetime()`을 사용한다.
- [to_datetime에 관한 자료](https://steadiness-193.tistory.com/171)
```python
dust_reviews["측정일시"] = dust_reviews["측정일시"].astype("str")
pd.to_datetime(dust_reviews["측정일시"], format="%Y$m%d")
dust_reviews.info()
```
---
- 인덱스를 설정하는 코드
- 인덱스를 다시 설정하기 위해서는 `df.set_index()`를 사용하면 된다.
- `inplace` 파라미터는 기존의 데이터프레임에 덮어쓴다는 의미이다.
```python
dust_reviews.set_index("측정일시", inplace=True)
```
---
- column의 형식이 카테고리인 경우 카테고리 목록의 수를 확인하기 위한 함수이다.
- `unique()`를 통해 종류를 확인한 후 그 길이를 구해도 된다.
- 따라서 연속형 변수인 경우에는 `unique`를 해도 의미가 없다.
---
- `describe`의 default는 `datatype`이 `number(int, float), datetime`일 때의 요약을 계산하는 함수이다. 
- 만일 내가 특정한 `datatype`에 대한 변수들만 출력하고 싶을 때에는 `include` 파라미터를 이용한다.
---
- 처음 n개의 데이터를 확인하기 위해서는 `head(n)`을 사용한다. (default, n=5)
-  맨 끝의 n개 데이터를 확인하기 위해서는 `tail(n)`을 사용한다. (default, n=5)
- 시계열 데이터의 경우에는 `head`,`tail`을 모두 확인해보는 경우가 많다. (<U>측정기간을 확인하기 위함</U>) [시계열데이터란?](https://ko.wikipedia.org/wiki/%EC%8B%9C%EA%B3%84%EC%97%B4)
---
- `value_counts()`는 특정 `column`이 카테고리일 때 빈도표를 구하기 위한 함수이다.
- `dust_reviews.value_count(subset="측정소명")`도 동일한 코드이다.
```python
# 같은 코드
dust_reviews['측정소명'].value_counts() 
dust_reviews.value_count(subset="측정소명")
```
- 멀티 인덱스를 표현할 수도 있다. `dust_reviews.value_counts(subset=["권역별","측정소명"])`
- 위의 코드 결과를 보면, 모든 측정소가 동일하게 <U>1년 동안의 모든 날짜에 대한 데이터 행이 존재함</U>을 알 수 있다.
---
- **피드백**
	- **데이터 정보 확인이 중요한 이유?**
		- 데이터 분석을 하기 전 데이터 전처리를 해야하는 데 어떤 부분을 변경해야 하는지 한번에 확인할 수 있다. [데이터 전처리란?](https://chaheekwon.tistory.com/entry/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC%EC%9D%98-%EA%B0%9C%EB%85%90%EA%B3%BC-%EC%A4%91%EC%9A%94%EC%84%B1)
		- 필요없는 column제거, datatype 변경 등의 처리를 할 수 있어야 한다.
		- 그 중 가장 중요하게 봐야할 요소는 **데이터 타입, 결측치 유무**이다.
### 결측치 파악하기
